
https://chrisalbon.com/machine-learning/feature_selection_using_random_forest.html
http://www.techpoweredmath.com/spark-dataframes-mllib-tutorial/#.Wh3bzlWnGpo
https://github.com/jakevdp/ESAC-stats-2014/blob/master/notebooks/04.2-Regression-Forests.ipynb
Feature importance -> https://stackoverflow.com/questions/28971989/pyspark-mllib-random-forest-feature-importances

Random forest- comparison
http://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/

Plotting example 
https://github.com/bigdata-i523/hid201/blob/master/experiment/Secchi%20Disk%20Data.ipynb

mkdir -p data
curl https://www.quandl.com/api/v3/datasets/BCHARTS/COINBASEUSD.csv?api_key=3ny-RLKDcGxjRDxRco8J >>data/Bitcoin_USD_History.csv
curl https://www.quandl.com/api/v3/datasets/BCHARTS/COINBASEGBP.csv?api_key=3ny-RLKDcGxjRDxRco8J >>data/Bitcoin_GBP_History.csv
curl https://www.quandl.com/api/v3/datasets/BCHARTS/COINBASEEUR.csv?api_key=3ny-RLKDcGxjRDxRco8J >>data/Bitcoin_EUR_History.csv

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

============rformula 
spark = SparkSession.builder.master("local").appName("Word Count")
...     .getOrCreate()


from pyspark.mllib.linalg import Vectors
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

dataFrame = sqlContext.createDataFrame([
    (0, Vectors.dense([1.0, 0.1, -1.0]),),
    (1, Vectors.dense([2.0, 1.1, 1.0]),),
    (2, Vectors.dense([3.0, 10.1, 3.0]),)
], ["id", "features"])


from pyspark.ml.feature import RFormula

dataset = sqlContext.createDataFrame(
    [(7, "US", 18, 1.0),
     (8, "CA", 12, 0.0),
     (9, "NZ", 15, 0.0)],
    ["id", "country", "hour", "clicked"])

	
============imputer

from pyspark.ml.feature import Imputer

df = sqlContext.createDataFrame([
    (1.0, float("nan")),
    (2.0, float("nan")),
    (float("nan"), 3.0),
    (4.0, 4.0),
    (5.0, 5.0)
], ["a", "b"])

imputer = Imputer(inputCols=["a", "b"], outputCols=["out_a", "out_b"])
model = imputer.fit(df)

model.transform(df).show()


==============indexer 
import numpy
from numpy import allclose
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import StringIndexer
from pyspark.ml.classification import RandomForestClassifier


stringIndexer = StringIndexer(inputCol="weekend", outputCol="indexed")
si_model = stringIndexer.fit(df)
td = si_model.transform(df)
rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol="indexed", seed=42)
model = rf.fit(td)
model.featureImportances

==================predict 

model = RandomForest.trainRegressor(sc.parallelize(sparse_data), {}, 2, seed=42)
 model.numTrees()
2
>>> model.totalNumNodes()
4
>>> model.predict(SparseVector(2, {1: 1.0}))
1.0
>>> model.predict(SparseVector(2, {0: 1.0}))
0.5
>>> rdd = sc.parallelize([[0.0, 1.0], [1.0, 0.0]])
>>> model.predict(rdd).collect()

























+++++++++++++++++++++++++++++++++++++++++++++++
from pyspark.sql.types import *
from pyspark.sql import Row
from datetime import datetime

import pyspark.mllib
import pyspark.mllib.regression
from pyspark.mllib.regression import LabeledPoint
from pyspark.sql.functions import *

data=sc.textFile('/idn/home/akuppura/learn/bit.tsv')
rdd = data.map(lambda line: line.split("\t"))
rdd.take(10)

df = rdd.map(lambda line: Row(Open = line[1], volume=line[5],weekday=datetime.strptime(line[0],"%y%m%d").date().weekday())).toDF()
temp = df.map(lambda line:LabeledPoint(line[0],[line[1:]]))
temp.take(5)
(trainingData, testData) = temp.randomSplit([0.9, 0.1])


model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo={},numTrees=10, featureSubsetStrategy="auto",impurity='variance', maxDepth=4, maxBins=32)
predictions = model.predict(testData.map(lambda x: x.features))
labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)
testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\
    float(testData.count())
print('Test Mean Squared Error = ' + str(testMSE))
print('Learned regression forest model:')
print(model.toDebugString())


model = GradientBoostedTrees.trainClassifier(trainingData,categoricalFeaturesInfo={}, numIterations=3)
predictions = model.predict(testData.map(lambda x: x.features))
labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)
testErr = labelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count() / float(testData.count())
print('Test Error = ' + str(testErr))
